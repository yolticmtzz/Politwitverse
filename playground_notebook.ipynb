{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/config.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\cb09766f7ba60b5f7a1bb640617b24f1499c4a6f3ab160c4a0ac171e3a377c68.008dca06003188334001a96363da79ced4944abc68d94a2f1e0db786dc5aa08b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/vocab.txt from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\973dbacfdf4c488622f01d1a226089e9e3dba130a0c3c11c2e36d49466fa40a8.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/bpe.codes from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\0e474c44ff353f3b378fb140e7e6d4431df4ec6142e8b38d584c0dbc5afc3521.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/added_tokens.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\fe46927817477a58ec2aa92ef52f8ee6fc9e824d054f4aa6a3c129724dc9c9b7.c1e7052e39d2135302ec27455f6db22e1520e6539942ff60a849c7f83f8ec6dc\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/special_tokens_map.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\9413ac0bed76140860deffa0c5a29ee4da7d49a3810da1b4b51b27f790bc9255.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/tokenizer_config.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\61374b71c02fdfd2929a3cdce24c242049e036624e15e18461a3a70cfc35e939.c260b44e952f7f2a825aac395f2ebbed4ac9553800d1e320af246e81a548f37c\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/config.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\cb09766f7ba60b5f7a1bb640617b24f1499c4a6f3ab160c4a0ac171e3a377c68.008dca06003188334001a96363da79ced4944abc68d94a2f1e0db786dc5aa08b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/config.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\cb09766f7ba60b5f7a1bb640617b24f1499c4a6f3ab160c4a0ac171e3a377c68.008dca06003188334001a96363da79ced4944abc68d94a2f1e0db786dc5aa08b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/pytorch_model.bin from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\2e4719cf8d097772eb75070b88cbc56f1d3b1392fffc5f75032a389ef21d1847.16366ca1277caccb15200478349503b3336a1420ac26d44fc16763354f5a2cae\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/vocab.txt from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\b7837213513a9f3852dcb04048f70c13cbd0590be030e534734ffd42cbdcf45a.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/bpe.codes from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\76e357e2554ebe053d1c4c613506bc2cc19d66ae27fec8218261a7f73c6456b9.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/added_tokens.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\c4b7522f44ed8adb95e62288c6458da591654f7466e3ce2f9c730bb4087411d2.c1e7052e39d2135302ec27455f6db22e1520e6539942ff60a849c7f83f8ec6dc\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/special_tokens_map.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\01581144d9bf96cb9c7d8a77ee93c8b1f1095af5c1204b1b038a8cb0e3247aa8.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/tokenizer_config.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\1740697312c59fe96586f476c7765cd6f08516a6102ea96f22ffee64f7553234.c260b44e952f7f2a825aac395f2ebbed4ac9553800d1e320af246e81a548f37c\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/pytorch_model.bin from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\61c5894a0aca5ed63159e2ec6a5501db48124c1e6de287b82bc634334f031203.9c3c4c16d0dd174434d42471b9d4670734d982be506a06fc3111c12bee4380c7\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/vocab.txt from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\af97f2d33aa35c368f65a032c5f6b065e8b88d9bdef502fbc2ce78e04579a0fa.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n",
      "loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/bpe.codes from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\c50391b2daa6a8c1be706382b1cd8d7f4996b5664c2a0bbab6c15097305ae731.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n",
      "loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/added_tokens.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\8715836fc39b559eca950b73d5a4651bf34ab8d3acb1d0b81aae6d76e1a38fac.c1e7052e39d2135302ec27455f6db22e1520e6539942ff60a849c7f83f8ec6dc\n",
      "loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/special_tokens_map.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\fdf2edf9c4b32d36848f3a7f4b9edd83fe65a7caa032a01e30f72cd87b62affc.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n",
      "loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/tokenizer_config.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\9eec04ca3ab9bdc071fafcf37e270203d932078b7715016c2df05a0f3bd1bf07.7cf9bfd7af087e7e2ea689b159647c3c8a45e7f0b92609dce7bca18d806905da\n",
      "loading file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/tokenizer.json from cache at None\n",
      "Adding <mask> to the vocabulary\n",
      "loading configuration file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/config.json from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\0c8f98b89844cfd90c61b9671ab81f30f6d2ffd807131275cc3d9fa6c8061b04.92562558566fffb3ff2db6e6250351c34cb7206924b30d0b25bc6447925d32c1\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"hateful\",\n",
      "    \"1\": \"targeted\",\n",
      "    \"2\": \"aggressive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"aggressive\": 2,\n",
      "    \"hateful\": 0,\n",
      "    \"targeted\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/bertweet-hate-speech/resolve/main/pytorch_model.bin from cache at C:\\Users\\Joe Wilson/.cache\\huggingface\\transformers\\b1b28641afa821c55bf1216f931286a1e2d6f2bdf6ce43893b82fd118ff62542.742154f1fb0df2aeaab5e0f8ce81baeb42a062ffddff7febde2369888239bf57\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from pysentimiento import create_analyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "# from pysentimiento.preprocessing import preprocess_tweet # using github preprocess instead\n",
    "import preprocessor as p\n",
    "import spacy\n",
    "import tweepy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "b_analyzer_sentiment = create_analyzer(task=\"sentiment\", lang=\"en\")\n",
    "b_analyzer_emotion = create_analyzer(task=\"emotion\", lang=\"en\")\n",
    "b_analyzer_hate = create_analyzer(task=\"hate_speech\", lang=\"en\")\n",
    "v_analyzer_sentiment = SentimentIntensityAnalyzer()\n",
    "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAGPIWwEAAAAANh02yZK%2Bg2Ga9OaIGmo%2FdcBKwI4%3DoBVTm4dbV9EsX06kTvtAz5XjSCK222TAxusnGUposUxAGoEFqg')\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG)\n",
    "TweetTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realtime Tweet Sentiment Analyzer using BERTWEET engine and customized ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_sentiment_analyzer(text):\n",
    "    \n",
    "    temp = b_analyzer_sentiment.predict(text)\n",
    "    bert_sentiment_label = temp.output # output = neg, pos, neu label\n",
    "    bert_sentiment_score = temp.probas # probas = percentage score\n",
    "    NEG = round(bert_sentiment_score.get('POS'), 2)\n",
    "    POS = round(bert_sentiment_score.get('NEG'), 2)\n",
    "    NEU = round(bert_sentiment_score.get('NEU'), 2)\n",
    "    score_probability = max(NEG, POS, NEU)\n",
    "    bert_sentiment_list = [bert_sentiment_label, score_probability]\n",
    "    \n",
    "    return(bert_sentiment_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realtime Tweet Tokenizer using nltk's tweet tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_tokenization(clean_tweet_text):\n",
    "    \n",
    "    token = TweetTokenizer()\n",
    "    temp_tokens = token.tokenize(clean_tweet_text)  \n",
    "    nltk_tokens = []\n",
    "    for w in temp_tokens:\n",
    "         if w not in stop_words:\n",
    "             if len(w) > 2:\n",
    "                 nltk_tokens.append(w)\n",
    "                 \n",
    "    return(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_emotion_analyzer(text):\n",
    "    \n",
    " temp = b_analyzer_emotion.predict(text)\n",
    " label = temp.output\n",
    " \n",
    " return (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_hate_analyzer(text):\n",
    "    \n",
    "     temp = b_analyzer_hate.predict(text)\n",
    "     temp_label = temp.output\n",
    "     if len(temp_label) > 2:\n",
    "          a = temp_label[0]\n",
    "          b = temp_label[1]\n",
    "          c = temp_label[2]\n",
    "          h = (a + \" \" + b + \" \" + c)\n",
    "     elif len(temp_label) == 2:\n",
    "          a = temp_label[0]\n",
    "          b = temp_label[1]\n",
    "          h = (a + \" \" + b)\n",
    "          \n",
    "     elif len(temp_label) == 1:\n",
    "          h = temp_label[0]\n",
    "     else:\n",
    "          h = \"None\"\n",
    "     \n",
    "     return (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "3\n",
      "------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "0\n",
      "------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "2\n",
      "------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "0\n",
      "------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "2\n",
      "------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "7\n",
      "------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "0\n",
      "------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "1\n",
      "------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "2\n",
      "------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "0\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAGPIWwEAAAAANh02yZK%2Bg2Ga9OaIGmo%2FdcBKwI4%3DoBVTm4dbV9EsX06kTvtAz5XjSCK222TAxusnGUposUxAGoEFqg')\n",
    "\n",
    "query = \"republicans -is:retweet lang:en\"\n",
    "# response = client.search_recent_tweets(query=query, tweet_fields=['author_id'], user_fields=['created_at', 'name'],\n",
    "#                               max_results=10)\n",
    "\n",
    "response = client.search_recent_tweets(query=query,tweet_fields=['attachments','author_id','context_annotations','conversation_id','created_at','entities','geo,id','in_reply_to_user_id','lang','possibly_sensitive','public_metrics','referenced_tweets','reply_settings','source','text','withheld'],user_fields=['created_at','description','entities,id','location','name','pinned_tweet_id','profile_image_url','protected,public_metrics','url','username','verified','withheld'],expansions=['attachments.poll_ids','attachments.media_keys','author_id','geo.place_id','in_reply_to_user_id','referenced_tweets.id','entities.mentions.username','referenced_tweets.id.author_id'],media_fields=['duration_ms','height','media_key', 'preview_image_url','promoted_metrics','public_metrics','type,url'],place_fields=['contained_within,country','country_code','full_name','geo,id','name','place_type'],poll_fields=['duration_minutes','end_datetime','id','options','voting_status'],max_results=10) \n",
    "\n",
    "for tweet in response.data: \n",
    "    # print(\"orginal tweet: \" + tweet.text)\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    clean_tweet_text = p.clean(tweet.text)\n",
    "    clean_tweet_text = clean_tweet_text.replace(\"&amp\", \"\")  \n",
    "    tweet_sentiment = tweet_sentiment_analyzer(tweet.text)\n",
    "    nltk_tokens = tweet_tokenization(clean_tweet_text) \n",
    "    tweet_emotion_label = tweet_emotion_analyzer(tweet.text)\n",
    "    tweet_hate_label = tweet_hate_analyzer(tweet.text)\n",
    "    # print(clean_tweet_text)\n",
    "    # print(tweet_sentiment)\n",
    "    # print(tweet_emotion_label)\n",
    "    # print(tweet_hate_label)\n",
    "    \n",
    "    print(type(tweet.context_annotations))\n",
    "    print(len(tweet.context_annotations))\n",
    "    print('------------------------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33db5c7e3bb784b844c2c176e5e511469c44d8edbdec04c9eaa139fbb1e863e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
